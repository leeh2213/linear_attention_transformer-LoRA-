å½“ç„¶å¯ä»¥ï¼ä»¥ä¸‹æ˜¯ä¸€ä¸ªé€‚ç”¨äºä½ çš„ `LoRASelfAttention` Python æ–‡ä»¶çš„ `README.md` ç¤ºä¾‹ï¼Œå‡è®¾è¿™æ˜¯ä¸€ä¸ªæ¨¡å—åŒ–å®ç° LoRAï¼ˆLow-Rank Adaptationï¼‰ç”¨äºæ³¨æ„åŠ›æœºåˆ¶çš„é¡¹ç›®ï¼š

---

# LoRASelfAttention

ä¸€ä¸ªè½»é‡çº§æ¨¡å—ï¼Œç”¨äºå°† Low-Rank Adaptation (LoRA) åº”ç”¨äº Transformer ä¸­çš„æ³¨æ„åŠ›å±‚ã€‚è¯¥å®ç°åŸºäº PyTorchï¼Œæ”¯æŒå¯¹ Queryã€Keyã€Value ä»¥åŠ Output æŠ•å½±åˆ†åˆ«æ³¨å…¥ LoRAã€‚

## âœ¨ ç‰¹æ€§

* âœ… æ”¯æŒè‡ªå®šä¹‰ LoRA çš„ç§© `r` ä¸ç¼©æ”¾å› å­ `lora_alpha`
* âœ… ç²¾ç»†æ§åˆ¶åœ¨å“ªäº›æŠ•å½±ä¸Šå¯ç”¨ LoRAï¼ˆå¦‚ `q`, `k`, `v`, `o`ï¼‰
* âœ… ä¸æ ‡å‡† Self-Attention å…¼å®¹ï¼Œå¯æ— ç¼é›†æˆè¿›ç°æœ‰ Transformer æ¶æ„
* âœ… æ”¯æŒå±€éƒ¨æ³¨æ„åŠ›çª—å£ã€ç›²åŒºæ³¨æ„åŠ›ï¼ˆBlindspot Attentionï¼‰ç­‰åŠŸèƒ½

---

## ğŸ§  ä»€ä¹ˆæ˜¯ LoRAï¼Ÿ

LoRAï¼ˆLow-Rank Adaptationï¼‰æ˜¯ä¸€ç§å‚æ•°é«˜æ•ˆçš„å¾®è°ƒæ–¹æ³•ï¼Œå®ƒé€šè¿‡åœ¨åŸå§‹æƒé‡çŸ©é˜µä¸¤ä¾§æ·»åŠ ä½ç§©çŸ©é˜µï¼Œä»è€Œåœ¨å†»ç»“åŸå§‹å‚æ•°çš„æƒ…å†µä¸‹å®ç°æ¨¡å‹è°ƒä¼˜ã€‚å…¶ä¼˜åŠ¿åœ¨äºæ˜¾è‘—å‡å°‘å¾®è°ƒæ—¶çš„å‚æ•°è§„æ¨¡ã€‚

è®ºæ–‡é“¾æ¥ï¼š[LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)

---

## ğŸ“¦ å®‰è£…ä¾èµ–

```bash
pip install torch
```

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

```python
from lora_self_attention import LoRASelfAttention

# æ„å»ºå¸¦æœ‰ LoRA çš„æ³¨æ„åŠ›å±‚
attn = LoRASelfAttention(
    dim=512,
    heads=8,
    r=8,
    lora_alpha=16,
    enable_lora=['q', 'k', 'v', 'o'],  # æ§åˆ¶åœ¨å“ªäº›æŠ•å½±ä¸Šä½¿ç”¨ LoRA
    causal=True,
    dim_head=64,
    blindspot_size=None,
    n_local_attn_heads=4,
    local_attn_window_size=128,
    attn_dropout=0.1,
    dropout=0.1
)

# è¾“å…¥å¼ é‡ï¼šbatch_size x sequence_length x embedding_dim
import torch
x = torch.randn(1, 128, 512)
out = attn(x)
print(out.shape)  # è¾“å‡ºå¤§å°åº”ä¸è¾“å…¥ä¸€è‡´
```

---

## âš™ï¸ å‚æ•°è¯´æ˜

| å‚æ•°å                      | æè¿°                                 | ç±»å‹            | é»˜è®¤å€¼                 |
| ------------------------ | ---------------------------------- | ------------- | ------------------- |
| `dim`                    | è¾“å…¥åµŒå…¥ç»´åº¦                             | `int`         | â€”                   |
| `heads`                  | å¤šå¤´æ³¨æ„åŠ›å¤´æ•°                            | `int`         | â€”                   |
| `r`                      | LoRA çš„ç§©ï¼ˆä½ç§©çŸ©é˜µç»´åº¦ï¼‰                    | `int`         | `8`                 |
| `lora_alpha`             | ç¼©æ”¾ç³»æ•°                               | `int`         | `16`                |
| `enable_lora`            | åº”ç”¨ LoRA çš„æŠ•å½±ï¼ˆå¦‚ `['q','k','v','o']`ï¼‰ | `list[str]`   | `['q','k','v','o']` |
| `causal`                 | æ˜¯å¦ä¸ºå› æœæ³¨æ„åŠ›ï¼ˆè‡ªå›å½’ï¼‰                      | `bool`        | `False`             |
| `dim_head`               | æ¯ä¸ªæ³¨æ„åŠ›å¤´çš„ç»´åº¦                          | `int`         | `64`                |
| `blindspot_size`         | ç›²åŒºæ³¨æ„åŠ›çš„çª—å£å¤§å°                         | `int or None` | `None`              |
| `n_local_attn_heads`     | ä½¿ç”¨å±€éƒ¨æ³¨æ„åŠ›çš„å¤´æ•°                         | `int`         | `0`                 |
| `local_attn_window_size` | å±€éƒ¨æ³¨æ„åŠ›çª—å£å¤§å°                          | `int`         | `128`               |
| `attn_dropout`           | æ³¨æ„åŠ›æƒé‡çš„ Dropout æ¦‚ç‡                  | `float`       | `0.1`               |
| `dropout`                | è¾“å‡ºæŠ•å½±åçš„ Dropout æ¦‚ç‡                  | `float`       | `0.1`               |

---

## ğŸ“ æ–‡ä»¶ç»“æ„

```bash
lora_self_attention/
â”œâ”€â”€ lora_self_attention.py  # æ ¸å¿ƒå®ç°æ–‡ä»¶
â”œâ”€â”€ README.md               # é¡¹ç›®è¯´æ˜æ–‡æ¡£
```

---
